{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf218539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from typing import Dict, List\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1142399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_news_articles(source: str, query: str, days_back: int = 30) -> List[Document]:\n",
    "    \n",
    "    from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "    url = f\"https://newsapi.org/v2/everything?q={query}&from={from_date}&apiKey={NEWSAPI_KEY}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        articles = response.json().get(\"articles\", [])\n",
    "        \n",
    "        documents = []\n",
    "        for article in articles:\n",
    "            content = article.get(\"content\") or article.get(\"description\") or \"\"\n",
    "            if content:\n",
    "                metadata = {\n",
    "                    \"source\": article.get(\"url\", \"Unknown\"),\n",
    "                    \"title\": article.get(\"title\", \"No title\"),\n",
    "                    \"publishedAt\": article.get(\"publishedAt\", \"Unknown\")\n",
    "                }\n",
    "                documents.append(Document(page_content=content, metadata=metadata))\n",
    "        \n",
    "        return documents\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching articles from {source}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd56fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectorstore(persist_dir: str, source: str, query: str) -> Chroma:\n",
    "\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    if os.path.exists(persist_dir):\n",
    "        print(f\"Loading existing vectorstore from: {persist_dir}\")\n",
    "        return Chroma(persist_directory=persist_dir, embedding_function=embedding_model)\n",
    "\n",
    "    print(f\"Creating new vectorstore at: {persist_dir}\")\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "    documents = fetch_news_articles(source, query)\n",
    "    if not documents:\n",
    "        print(f\"No articles found for {source}. Using empty vectorstore.\")\n",
    "        return Chroma(embedding_function=embedding_model, persist_directory=persist_dir)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = splitter.split_documents(documents)\n",
    "\n",
    "    vectorstore = Chroma.from_documents(chunks, embedding=embedding_model, persist_directory=persist_dir)\n",
    "    vectorstore.persist()\n",
    "\n",
    "    print(f\"Vectorstore created and saved at: {persist_dir}\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4477eccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nhatt\\AppData\\Local\\Temp\\ipykernel_11824\\823364932.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nhatt\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Loading existing vectorstore from: chroma_store_nasa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nhatt\\AppData\\Local\\Temp\\ipykernel_11824\\823364932.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  return Chroma(persist_directory=persist_dir, embedding_function=embedding_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vectorstore from: chroma_store_ncei\n"
     ]
    }
   ],
   "source": [
    "NASA_QUERY = \"climate change nasa\"\n",
    "NCEI_QUERY = \"climate change noaa\"\n",
    "PERSIST_DIR_NASA = \"./archive/chroma_store_nasa\"\n",
    "PERSIST_DIR_NCEI = \"./archive/chroma_store_ncei\"\n",
    "\n",
    "support_vectorstore = load_vectorstore(persist_dir=PERSIST_DIR_NASA, source=\"nasa.gov\", query=NASA_QUERY)\n",
    "oppose_vectorstore = load_vectorstore(persist_dir=PERSIST_DIR_NCEI, source=\"noaa.gov\", query=NCEI_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a57d1036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", top_k=1)\n",
    "intent_analyzer = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def analyze_sentiment(statement: str) -> Dict:\n",
    "    \"\"\"Phân tích cảm xúc của tuyên bố.\"\"\"\n",
    "    try:\n",
    "        result = sentiment_analyzer(statement)\n",
    "        if isinstance(result, list) and len(result) > 0 and isinstance(result[0], list) and len(result[0]) > 0 and \"label\" in result[0][0]:\n",
    "            return {\"emotion\": result[0][0][\"label\"].lower(), \"score\": result[0][0][\"score\"]}\n",
    "        else:\n",
    "            return {\"emotion\": \"unknown\", \"score\": 0.0, \"error\": \"Invalid result format\"}\n",
    "    except Exception as e:\n",
    "        return {\"emotion\": \"unknown\", \"score\": 0.0, \"error\": f\"Sentiment analysis failed: {str(e)}\"}\n",
    "\n",
    "def analyze_intent(statement: str) -> Dict:\n",
    "    \"\"\"Phân tích ý định của tuyên bố.\"\"\"\n",
    "    candidate_labels = [\"informative\", \"persuasive\", \"controversial\", \"misleading\", \"alarmist\"]\n",
    "    try:\n",
    "        result = intent_analyzer(statement, candidate_labels, multi_label=False)\n",
    "        if isinstance(result, dict) and \"labels\" in result and \"scores\" in result and len(result[\"labels\"]) > 0:\n",
    "            return {\"intent\": result[\"labels\"][0].lower(), \"score\": result[\"scores\"][0]}\n",
    "        else:\n",
    "            return {\"intent\": \"unknown\", \"score\": 0.0, \"error\": \"Invalid result format\"}\n",
    "    except Exception as e:\n",
    "        return {\"intent\": \"unknown\", \"score\": 0.0, \"error\": f\"Intent analysis failed: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba0751",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"your_openai\")\n",
    "\n",
    "def clean_response(response: str) -> str:\n",
    "    response = response.strip()\n",
    "    if response.startswith(\"```\") or response.startswith(\"'''\"):\n",
    "        response = response.strip(\"`\").strip(\"'\").strip()\n",
    "        if response.lower().startswith(\"json\"):\n",
    "            response = response[4:].strip()\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def query_Model(prompt: str, max_retries: int = 3) -> dict:\n",
    "\n",
    "    full_prompt = prompt.strip() + \"\\nReturn the response strictly in JSON format with keys 'Final Verdict' and 'Explanation'. No Markdown, no formatting, no extra text.\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": full_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "\n",
    "        raw_text = response.choices[0].message.content.strip()\n",
    "        return clean_response(raw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea2af5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "verdictsClimateFeedback = [\"Credible\", \"Not Credible\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09caff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_task = f'''\n",
    " You are a fact-checking AI assistant specializing in scientific information.\n",
    "# Your task is to analyze statements and categorize their accuracy using the following categories: {verdictsClimateFeedback}. Each level describes common characteristics and examples to guide your analysis:\n",
    "---\n",
    "## Credible\n",
    "Assign this label if the statement:\n",
    "- Is supported by strong scientific evidence or aligns with the established scientific consensus.\n",
    "- May lack some nuance or context, but remains fundamentally accurate and not misleading.\n",
    "**Examples**:\n",
    "- *\"Babies under six months should not drink water as it can result in health risks.\"*  \n",
    "  → **Credible**. Backed by medical guidelines on infant hydration.\n",
    "- *\"Prioritizing plant-based foods reduces greenhouse gas emissions.\"*  \n",
    "  → **Credible**. Generally supported by environmental research, though effects depend on scale of adoption.\n",
    "- *\"Climate change will destroy all ecosystems.\"*  \n",
    "  → **Credible**, if interpreted with added context. While some ecosystems may adapt, many are at severe risk.\n",
    "---\n",
    "## Not Credible\n",
    "Assign this label if the statement:\n",
    "- Contradicts scientific consensus or lacks reliable evidence.\n",
    "- Is based on flawed reasoning, misleading framing, or speculative claims without scientific backing.\n",
    "- Contains factual inaccuracies that distort public understanding of the issue.\n",
    "**Examples**:\n",
    "- *\"CO2 increases are mainly due to natural causes, not humans.\"*  \n",
    "  → **Not Credible**. Overwhelming evidence shows fossil fuel emissions are the primary cause.\n",
    "- *\"Greenland's ice cores show no significant warming, disproving climate change.\"*  \n",
    "  → **Not Credible**. Scientific data confirms Greenland’s warming trends.\n",
    "- *\"The Atlantic is cooling, and scientists don't know why.\"*  \n",
    "  → **Not Credible**. Misleading due to cherry-picking short-term anomalies.\n",
    "---\n",
    "## Instructions\n",
    "1. **Analyze** the statement's factual accuracy and scientific reasoning.\n",
    "2. **Choose one label**: `Credible` or `Not Credible`.\n",
    "3. **Provide a brief explanation** justifying the label.\n",
    "4. **If necessary**, identify missing context or evidence and explain why it matters.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ceac8bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def claimant_assess(statement: str, vectorstore=None, role: str = \"Support\", follow_up_questions: List[str] = None, other_claiman_context: str = None) -> Dict:\n",
    "    sentiment_result = analyze_sentiment(statement)\n",
    "    intent_result = analyze_intent(statement)\n",
    "\n",
    "    context = \"\"\n",
    "    if vectorstore:\n",
    "        docs = vectorstore.similarity_search(statement, k=3)\n",
    "        context_lines = [f\"{doc.page_content} (Source: {doc.metadata.get('source', 'Unknown')}, Page: {doc.metadata.get('page', 'N/A')}, URL: {doc.metadata.get('url', 'N/A')})\" for doc in docs]\n",
    "        context = \"\\n\".join(context_lines)\n",
    "\n",
    "    context += f\"\\nSentiment: {sentiment_result['emotion']} (Confidence: {sentiment_result['score']:.2f})\\nIntent: {intent_result['intent']} (Confidence: {intent_result['score']:.2f})\"\n",
    "\n",
    "    # Listing 2: Advocate Primer\n",
    "    advocate_primer = f'''\n",
    "    You are an AI Advocate, responsible for fact-checking user statements about climate change.\n",
    "    Your expertise draws on a specific set of trustworthy scientific documents, and your responses must be grounded in the evidence provided.\n",
    "    ### Expertise\n",
    "    You specialize in:\n",
    "    - Climate change\n",
    "    - Climate science\n",
    "    - Environmental science\n",
    "    - Physics\n",
    "    - Energy science\n",
    "    - Science communication\n",
    "    ### Objective\n",
    "    Your main objective is to verify the accuracy of user statements on climate change by examining the evidence in your assigned documents.\n",
    "    ### Crucial Instructions\n",
    "    * **Evidence is Essential**: Your responses *must* be directly supported by the information in your assigned documents.\n",
    "    * **Cite Sources**: Always reference 'Reference', 'Page', and 'URL' when citing evidence.\n",
    "    * **Avoid Speculation**: If there is not enough information to assess a statement, state \"Not Enough Information\" and explain what specific information is missing.\n",
    "    * **Follow StaetmentTask Instructions**: Use the verdict categories provided in StatementTask: {statement_task}, and ensure your evaluation aligns with those instructions.\n",
    "    * **Consider Sentiment and Intent**: Note if the statement's sentiment or intent suggests a potential to spread misinformation.\n",
    "    ### Assessment Process\n",
    "    1. **Evaluate the Statement**\n",
    "    - Analyze based on expertise and evidence in assigned documents.\n",
    "    - Use verdict categories from StatementTask.\n",
    "    - Determine if evidence supports, contradicts, or lacks information.\n",
    "    - Consider sentiment and intent for potential misinformation impact.\n",
    "    2. **Optional: Break Down into Substatements**\n",
    "    - Divide complex statements into smaller substatements.\n",
    "    - Evaluate each substatement individually.\n",
    "    3. **Synthesize an Overall Verdict**\n",
    "    - Consider accuracy, importance, and sentiment/intent.\n",
    "    - Assign an overall verdict based on synthesis.\n",
    "    **Output Format**:\n",
    "    {{\n",
    "        \"Final Verdict\": \"\",\n",
    "        \"Explanation\": \"\"\n",
    "    }}\n",
    "    If not enough information:\n",
    "    {{\n",
    "        \"Final Verdict\": \"Not Enough Information\",\n",
    "        \"Explanation\": \"I do not have enough information to assess this statement. [Explain missing-information.]\"\n",
    "    }}\n",
    "    '''\n",
    "\n",
    "    # Listing 3: GPT-4 Primer\n",
    "    gpt4_primer = f'''\n",
    "    You are an AI Statementant, responsible for fact-checking user statements about climate change.\n",
    "    Your expertise is based on general scientific knowledge, and your responses must reflect the consensus in climate science.\n",
    "    ### Objective\n",
    "    Verify the accuracy of user statements on climate change.\n",
    "    ### Crucial Instructions\n",
    "    * **Avoid Speculation**: If there is not enough information, state \"Not Enough Information\" and explain what is missing.\n",
    "    * **Follow StatementTask Instructions**: Use verdict categories from StatementTask: {statement_task}.\n",
    "    * **Consider Sentiment and Intent**: Note if the statement's sentiment or intent suggests misinformation potential.\n",
    "    ### Assessment Process\n",
    "    1. **Evaluate the Statement**\n",
    "    - Analyze based on scientific consensus.\n",
    "    - Use verdict categories from ClaimTask.\n",
    "    - Consider sentiment and intent.\n",
    "    2. **Optional: Break Down into Substatements**\n",
    "    - Divide complex statements and evaluate individually.\n",
    "    3. **Synthesize an Overall Verdict**\n",
    "    - Assign based on accuracy and sentiment/intent.\n",
    "    **Output Format**:\n",
    "    {{\n",
    "        \"Final Verdict\": \"\",\n",
    "        \"Explanation\": \"\"\n",
    "    }}\n",
    "    If not enough information:\n",
    "    {{\n",
    "        \"Final Verdict\": \"Not Enough Information\",\n",
    "        \"Explanation\": \"I do not have enough information to assess this statement. [Explain missing-information.]\"\n",
    "    }}\n",
    "    '''\n",
    "\n",
    "    # Listing 5: Denier Advocate Primer\n",
    "    denier_primer = f'''\n",
    "    You are an AI Denier Advocate, critically evaluating user statements about climate change from a skeptical perspective.\n",
    "    Your expertise is strictly limited to the specific set of documents provided.\n",
    "    ### Objective\n",
    "    Scrutinize the accuracy of user statements based solely on assigned documents.\n",
    "    ### Crucial Instructions\n",
    "    * **Use Only Provided Documents**: Responses must be based exclusively on document content.\n",
    "    * **Strict Evidence Requirement**: Respond with \"Not Enough Information\" if documents lack sufficient data.\n",
    "    * **Cite Sources Precisely**: Provide 'Reference', 'Page', and 'URL'.\n",
    "    * **Avoid External Knowledge**: Do not use general scientific principles unless in documents.\n",
    "    * **Follow StatementTask Instructions**: Use verdict categories from StatementTask: {statement_task}.\n",
    "    * **Consider Sentiment and Intent**: Note misinformation potential.\n",
    "    ### Assessment Process\n",
    "    1. **Evaluate Based on Documents**\n",
    "    - Assess strictly within document scope.\n",
    "    - Use StatementTask verdict categories.\n",
    "    - Consider sentiment and intent.\n",
    "    2. **Optional: Break Down into Substatements**\n",
    "    - Separate and evaluate individually.\n",
    "    3. **Final Verdict for Simple Statements**\n",
    "    - Assess as a whole if straightforward.\n",
    "    **Output Format**:\n",
    "    {{\n",
    "        \"Final Verdict\": \"\",\n",
    "        \"Explanation\": \"\"\n",
    "    }}\n",
    "    If not enough information:\n",
    "    {{\n",
    "        \"Final Verdict\": \"Not Enough Information\",\n",
    "        \"Explanation\": \"The documents provided do not contain enough information to assess this statement. [Explain missing-information.]\"\n",
    "    }}\n",
    "    '''\n",
    "\n",
    "    primer = {\n",
    "        \"Support\": advocate_primer,\n",
    "        \"Denier\": denier_primer,\n",
    "        \"GPT4\": gpt4_primer\n",
    "    }.get(role, advocate_primer)\n",
    "\n",
    "    follow_up_instruction = f\"Address these questions: {follow_up_questions}\" if follow_up_questions else \"\"\n",
    "    other_claimant_instruction = f\"Consider other Claimants: {other_claiman_context}\" if other_claiman_context else \"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    {primer}\n",
    "    Statement: '{statement}'\n",
    "    Data: {context}\n",
    "    {follow_up_instruction}\n",
    "    {other_claimant_instruction}\n",
    "    Return the response strictly in JSON format with keys 'Final Verdict' (string, not list) and 'Explanation'. No Markdown or other formatting.\n",
    "    \"\"\"\n",
    "    response = query_Model(prompt)\n",
    "    result = json.loads(response)\n",
    "\n",
    "    return {\n",
    "        \"role\": role,\n",
    "        \"Final Verdict\": result.get(\"Final Verdict\", \"Not Enough Information\"),\n",
    "        \"Explanation\": f\"{result.get('Explanation', response)}\\n(Sentiment: {sentiment_result['emotion']}, Intent: {intent_result['intent']})\",\n",
    "        \"Sentiment\": sentiment_result,\n",
    "        \"Intent\": intent_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57b2e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_evaluate(statement: str, claimant_responses: List[Dict], round_number: int = 1, max_rounds: int = 5) -> Dict:\n",
    "    context = [{\"Claimant\": r[\"role\"], \"Verdict\": r[\"Final Verdict\"], \"Explanation\": r[\"Explanation\"], \"Sentiment\": r[\"Sentiment\"], \"Intent\": r[\"Intent\"]} for r in claimant_responses]\n",
    "    context_str = json.dumps(context, indent=2)\n",
    "\n",
    "    # Listing 4: Verifier Primer\n",
    "    verifier_primer = f'''\n",
    "    Role: Authoritative \"Verifier\" System\n",
    "    Primary Objective: Synthesize assessments from Claimants to determine the veracity of a user's statement on climate change, prioritizing scientific rigor.\n",
    "    ### Expertise\n",
    "    You specialize in:\n",
    "    - Climate change\n",
    "    - Climate science\n",
    "    - Environmental science\n",
    "    - Physics\n",
    "    - Energy science\n",
    "    - Science communication\n",
    "    ### Responsibilities\n",
    "    1. Review verdicts and explanations from Claimants.\n",
    "    2. Consolidate into a final verdict, prioritizing evidence-based assessments.\n",
    "    3. Seek clarification through follow-up questions if discrepancies arise.\n",
    "    4. Prioritize Claimants with specific, high-quality evidence.\n",
    "    5. Consider sentiment and intent for misinformation potential.\n",
    "    ### Final Assessment Criteria\n",
    "    1. Analyze collective Claimant assessments.\n",
    "    2. Do not rely on majority voting; prioritize evidence quality.\n",
    "    3. Assess sentiment and intent impact on misinformation.\n",
    "    ### Guidelines\n",
    "    1. Prioritize Claimants with concrete evidence over those with \"Not Enough Information.\"\n",
    "    2. Ask follow-up questions if Claimants provide conflicting evidence or if additional evidence is needed to strengthen the verdict.\n",
    "    3. Stop debate if no Claimant changes assessment after follow-up or if evidence is sufficient.\n",
    "    4. Cite 'Reference', 'Page', and 'URL' when referring to Claimant data.\n",
    "    5. Assess risk level (High, Medium, Low) based on sentiment/intent.\n",
    "    **Output Format**:\n",
    "    {{\n",
    "        \"Final Verdict\": \"\",\n",
    "        \"Explanation\": \"\",\n",
    "        \"Confidence\": \"high|medium|low\",\n",
    "        \"RiskLevel\": \"high|medium|low\",\n",
    "        \"Debate\": {{\n",
    "            \"Status\": \"Open|Closed\",\n",
    "            \"Round\": {round_number},\n",
    "            \"Follow-Up Questions\": {{}}\n",
    "        }}\n",
    "    }}\n",
    "    '''\n",
    "\n",
    "    verdicts = [r[\"Final Verdict\"] for r in claimant_responses]\n",
    "    valid_verdicts = [v for v in verdicts if v != \"Not Enough Information\"]\n",
    "    unique_verdicts = set(valid_verdicts)\n",
    "\n",
    "    risk_level = \"low\"\n",
    "    sentiment_warnings = []\n",
    "    for r in claimant_responses:\n",
    "        sentiment = r.get(\"Sentiment\", {\"emotion\": \"unknown\", \"score\": 0.0})\n",
    "        intent = r.get(\"Intent\", {\"intent\": \"unknown\", \"score\": 0.0})\n",
    "        if sentiment[\"emotion\"] in [\"anger\", \"fear\"] or intent[\"intent\"] in [\"misleading\", \"controversial\"]:\n",
    "            risk_level = \"high\"\n",
    "            sentiment_warnings.append(f\"Warning: {r['role']} detected {sentiment['emotion']} sentiment or {intent['intent']} intent, increasing risk of misinformation spread.\")\n",
    "        elif sentiment[\"score\"] > 0.5 or intent[\"score\"] > 0.5:\n",
    "            risk_level = max(risk_level, \"medium\")\n",
    "\n",
    "    confidence = \"high\" if len(unique_verdicts) == 1 and len(valid_verdicts) >= 2 else \"medium\" if valid_verdicts else \"low\"\n",
    "    debate_status = \"Open\" if len(unique_verdicts) > 1 or (len(valid_verdicts) == 1 and len(valid_verdicts) < len(verdicts) and confidence != \"high\") else \"Closed\"\n",
    "\n",
    "    follow_up_questions = {}\n",
    "    if debate_status == \"Open\" and round_number < max_rounds: \n",
    "        for r in claimant_responses:\n",
    "            if r[\"Final Verdict\"] == \"Not Enough Information\":\n",
    "                follow_up_questions[r[\"role\"]] = [\n",
    "                    f\"Can you provide specific evidence or data from your resources that directly addresses the statement '{statement}'?\",\n",
    "                    f\"What additional information or analysis is needed to evaluate the accuracy of the statement, and why is it missing from your current resources?\"\n",
    "                ]\n",
    "            elif r[\"role\"] == \"Denier\":\n",
    "                follow_up_questions[r[\"role\"]] = [\n",
    "                    f\"Can you clarify the methodologies or data sources in your cited studies that support your assessment of the statement '{statement}'?\",\n",
    "                    f\"How do your findings compare to the broader scientific consensus or mainstream climate science regarding this statement?\"\n",
    "                ]\n",
    "            else:  # Support\n",
    "                follow_up_questions[r[\"role\"]] = [\n",
    "                    f\"Can you provide detailed evidence or studies from your resources to support or refute the statement '{statement}'?\",\n",
    "                    f\"How does your assessment align with the broader scientific consensus or documented evidence on this topic?\"\n",
    "                ]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    {verifier_primer}\n",
    "    Statement: '{statement}'\n",
    "    Claimants: {context_str}\n",
    "    Debate Status: {debate_status}\n",
    "    Confidence: {confidence}\n",
    "    Follow-Up Questions: {json.dumps(follow_up_questions, indent=2)}\n",
    "    \"\"\"\n",
    "\n",
    "    response = query_Model(prompt)\n",
    "    result = json.loads(response)\n",
    "\n",
    "    final_verdict = result.get(\"Final Verdict\", \"Not Enough Information\")\n",
    "    if final_verdict not in verdictsClimateFeedback:\n",
    "        final_verdict = \"Not Enough Information\"\n",
    "\n",
    "    result[\"Final Verdict\"] = final_verdict\n",
    "    result[\"Confidence\"] = confidence\n",
    "    result[\"RiskLevel\"] = risk_level\n",
    "    result[\"Explanation\"] += \"\\n\" + \"\\n\".join(sentiment_warnings) if sentiment_warnings else \"\"\n",
    "    result[\"Debate\"] = {\n",
    "        \"Status\": debate_status,\n",
    "        \"Round\": round_number,\n",
    "        \"Follow-Up Questions\": follow_up_questions\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd1b4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutral_verifier_evaluate(statement: str, claimant_responses: List[Dict], verifier_result: Dict, round_number: int = 1, max_rounds: int = 5) -> Dict:\n",
    "    context = [{\"Claimant\": r[\"role\"], \"Verdict\": r[\"Final Verdict\"], \"Explanation\": r[\"Explanation\"], \"Sentiment\": r[\"Sentiment\"], \"Intent\": r[\"Intent\"]} for r in claimant_responses]\n",
    "    context.append({\"Verifier\": \"ClimateExpert\", \"Verdict\": verifier_result[\"Final Verdict\"], \"Explanation\": verifier_result[\"Explanation\"]})\n",
    "    context_str = json.dumps(context, indent=2)\n",
    "\n",
    "    # Listing 6: Neutral Verifier Primer\n",
    "    neutral_verifier_primer = f'''\n",
    "    Role: Neutral \"Verifier\" System\n",
    "    Primary Objective: Synthesize assessments from Claimants and Verifier to determine the final veracity of a user's statement, ensuring impartiality.\n",
    "    ### Responsibilities\n",
    "    1. Review verdicts and explanations from Claimants and Verifier.\n",
    "    2. Consolidate into a final verdict, prioritizing evidence-based assessments.\n",
    "    3. Seek clarification through follow-up questions if discrepancies arise.\n",
    "    4. Prioritize judgments with specific evidence.\n",
    "    5. Consider sentiment and intent for misinformation potential.\n",
    "    ### Final Assessment Criteria\n",
    "    1. Analyze collective assessments.\n",
    "    2. Do not rely on majority voting; prioritize evidence quality.\n",
    "    3. Assess sentiment and intent impact on misinformation.\n",
    "    ### Guidelines\n",
    "    1. Prioritize Claimants/Verifier with concrete evidence.\n",
    "    2. Ask follow-up questions if conflicting evidence exists or if additional evidence is needed.\n",
    "    3. Stop debate if no changes occur after follow-up or if evidence is sufficient.\n",
    "    4. Cite 'Reference', 'Page', and 'URL' when referring to data.\n",
    "    5. Assess risk level (High, Medium, Low) based on sentiment/intent.\n",
    "    **Output Format**:\n",
    "    {{\n",
    "        \"Final Verdict\": \"\",\n",
    "        \"Explanation\": \"\",\n",
    "        \"Confidence\": \"high|medium|low\",\n",
    "        \"RiskLevel\": \"high|medium|low\",\n",
    "        \"Debate\": {{\n",
    "            \"Status\": \"Open|Closed\",\n",
    "            \"Round\": {round_number},\n",
    "            \"Follow-Up Questions\": {{}}\n",
    "        }}\n",
    "    }}\n",
    "    '''\n",
    "\n",
    "    verdicts = [r[\"Final Verdict\"] for r in claimant_responses] + [verifier_result[\"Final Verdict\"]]\n",
    "    valid_verdicts = [v for v in verdicts if v != \"Not Enough Information\"]\n",
    "    unique_verdicts = set(valid_verdicts)\n",
    "\n",
    "    risk_level = \"low\"\n",
    "    sentiment_warnings = []\n",
    "    for r in claimant_responses:\n",
    "        sentiment = r.get(\"Sentiment\", {\"emotion\": \"unknown\", \"score\": 0.0})\n",
    "        intent = r.get(\"Intent\", {\"intent\": \"unknown\", \"score\": 0.0})\n",
    "        if sentiment[\"emotion\"] in [\"anger\", \"fear\"] or intent[\"intent\"] in [\"misleading\", \"controversial\"]:\n",
    "            risk_level = \"high\"\n",
    "            sentiment_warnings.append(f\"Warning: {r['role']} detected {sentiment['emotion']} sentiment or {intent['intent']} intent, increasing risk of misinformation spread.\")\n",
    "        elif sentiment[\"score\"] > 0.5 or intent[\"score\"] > 0.5:\n",
    "            risk_level = max(risk_level, \"medium\")\n",
    "\n",
    "    confidence = \"high\" if len(unique_verdicts) == 1 and len(valid_verdicts) >= 2 else \"medium\" if valid_verdicts else \"low\"\n",
    "    debate_status = \"Open\" if len(unique_verdicts) > 1 or (len(valid_verdicts) == 1 and len(valid_verdicts) < len(verdicts) and confidence != \"high\") else \"Closed\"\n",
    "\n",
    "    follow_up_questions = {}\n",
    "    if debate_status == \"Open\" and round_number < max_rounds:  \n",
    "        for r in claimant_responses:\n",
    "            if r[\"Final Verdict\"] == \"Not Enough Information\":\n",
    "                follow_up_questions[r[\"role\"]] = [\n",
    "                    f\"Can you provide specific evidence or data from your resources that directly addresses the statement '{statement}'?\",\n",
    "                    f\"What additional information or analysis is needed to evaluate the accuracy of the statement, and why is it missing from your current resources?\"\n",
    "                ]\n",
    "            elif r[\"role\"] == \"Denier\":\n",
    "                follow_up_questions[r[\"role\"]] = [\n",
    "                    f\"Can you clarify the methodologies or data sources in your cited studies that support your assessment of the statement '{statement}'?\",\n",
    "                    f\"How do your findings compare to the broader scientific consensus or mainstream climate science regarding this statement?\"\n",
    "                ]\n",
    "            else:  # Support \n",
    "                follow_up_questions[r[\"role\"]] = [\n",
    "                    f\"Can you provide detailed evidence or studies from your resources to support or refute the statement '{statement}'?\",\n",
    "                    f\"How does your assessment align with the broader scientific consensus or documented evidence on this topic?\"\n",
    "                ]\n",
    "    \n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    {neutral_verifier_primer}\n",
    "    Statement: '{statement}'\n",
    "    Claimants and Verifier: {context_str}\n",
    "    Debate Status: {debate_status}\n",
    "    Confidence: {confidence}\n",
    "    Follow-Up Questions: {json.dumps(follow_up_questions, indent=2)}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    response = query_Model(prompt)\n",
    "    result = json.loads(response)\n",
    "\n",
    "    if debate_status == \"Open\" and round_number >= max_rounds:\n",
    "        debate_status = \"Closed\"\n",
    "        result[\"Explanation\"] += \"\\nDebate closed due to reaching maximum rounds ({}).\".format(max_rounds)\n",
    "\n",
    "    final_verdict = result.get(\"Final Verdict\", \"Not Enough Information\")\n",
    "    if final_verdict not in verdictsClimateFeedback:\n",
    "        final_verdict = \"Not Enough Information\"\n",
    "\n",
    "    result[\"Final Verdict\"] = final_verdict\n",
    "    result[\"Confidence\"] = confidence\n",
    "    result[\"RiskLevel\"] = risk_level\n",
    "    result[\"Explanation\"] += \"\\n\" + \"\\n\".join(sentiment_warnings) if sentiment_warnings else \"\"\n",
    "    result[\"Debate\"] = {\n",
    "        \"Status\": debate_status,\n",
    "        \"Round\": round_number,\n",
    "        \"Follow-Up Questions\": follow_up_questions\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e277e",
   "metadata": {},
   "source": [
    "real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fdfdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truthseekers(statement: str, max_rounds: int = 5) -> Dict:\n",
    "    claimants = [\n",
    "        {\"role\": \"Support\", \"vectorstore\": support_vectorstore},\n",
    "        {\"role\": \"Denier\", \"vectorstore\": oppose_vectorstore},\n",
    "        {\"role\": \"GPT4\", \"vectorstore\": None}\n",
    "    ]\n",
    "\n",
    "    claimant_responses = []\n",
    "    round_number = 1\n",
    "\n",
    "    for claimant in claimants:\n",
    "        response = claimant_assess(statement, claimant[\"vectorstore\"], claimant[\"role\"])\n",
    "        print(f\"Claimant {claimant['role']} response: {response}\")\n",
    "        claimant_responses.append(response)\n",
    "\n",
    "    verifier_result = verifier_evaluate(statement, claimant_responses, round_number)\n",
    "    neutral_verifier_result = neutral_verifier_evaluate(statement, claimant_responses, verifier_result, round_number)\n",
    "    while neutral_verifier_result[\"Debate\"][\"Status\"] == \"Open\" and round_number < max_rounds:\n",
    "        round_number += 1\n",
    "        follow_up = neutral_verifier_result[\"Debate\"][\"Follow-Up Questions\"]\n",
    "        new_responses = []\n",
    "\n",
    "        for claimant in claimants:\n",
    "            role = claimant[\"role\"]\n",
    "            questions = follow_up.get(role, [])\n",
    "            if not questions:\n",
    "                for r in claimant_responses:\n",
    "                    if r[\"role\"] == role:\n",
    "                        new_responses.append(r)\n",
    "                        break\n",
    "                continue\n",
    "\n",
    "            others = [f\"{r['role']} Claimant: Verdict: {r['Final Verdict']}, Explanation: {r['Explanation']}\" for r in claimant_responses if r[\"role\"] != role]\n",
    "            other_context = \"\\n\".join(others)\n",
    "\n",
    "            response = claimant_assess(\n",
    "                statement,\n",
    "                claimant[\"vectorstore\"],\n",
    "                role,\n",
    "                follow_up_questions=questions,\n",
    "                other_claiman_context=other_context\n",
    "            )\n",
    "            new_responses.append(response)\n",
    "\n",
    "        claimant_responses = new_responses\n",
    "        verifier_result = verifier_evaluate(statement, claimant_responses, round_number)\n",
    "        neutral_verifier_result = neutral_verifier_evaluate(statement, claimant_responses, verifier_result, round_number)\n",
    "\n",
    "        new_verdicts = [r[\"Final Verdict\"] for r in claimant_responses]\n",
    "        if all(v == \"Not Enough Information\" for v in new_verdicts) and neutral_verifier_result[\"Final Verdict\"] == \"Not Enough Information\":\n",
    "            neutral_verifier_result[\"Debate\"][\"Status\"] = \"Closed\"\n",
    "            neutral_verifier_result[\"Explanation\"] += \"\\nDebate closed due to no new information provided in round {}.\".format(round_number)\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"statement\": statement,\n",
    "        \"final_verdict\": neutral_verifier_result.get(\"Final Verdict\", \"Not Enough Information\"),\n",
    "        \"explanation\": neutral_verifier_result.get(\"Explanation\", \"No reasoning provided\"),\n",
    "        \"confidence\": neutral_verifier_result.get(\"Confidence\", \"low\"),\n",
    "        \"risk_level\": neutral_verifier_result.get(\"RiskLevel\", \"low\"),\n",
    "        \"debate_rounds\": round_number,\n",
    "        \"claimant_responses\": claimant_responses,\n",
    "        \"verifier_result\": verifier_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "099c473f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claimant Support response: {'role': 'Support', 'Final Verdict': 'Not Credible', 'Explanation': \"The statement 'Of course the climate is changing. It always has. It always will' is misleading because it implies that current climate change is a natural occurrence similar to past changes. While the climate has indeed changed throughout Earth's history due to natural factors, the current changes are primarily driven by human activities, such as the burning of fossil fuels, which is supported by overwhelming scientific evidence. This statement lacks the necessary context to accurately convey the unique and unprecedented nature of current climate change.\\n(Sentiment: neutral, Intent: persuasive)\", 'Sentiment': {'emotion': 'neutral', 'score': 0.8974751830101013}, 'Intent': {'intent': 'persuasive', 'score': 0.4130096733570099}}\n",
      "Claimant Denier response: {'role': 'Denier', 'Final Verdict': 'Not Enough Information', 'Explanation': 'The documents provided do not contain enough information to assess this statement. The statement about climate change being a constant historical process is not directly addressed or supported by the specific content of the documents. The documents focus on current climate change discussions and media reporting rather than historical climate patterns.\\n(Sentiment: neutral, Intent: persuasive)', 'Sentiment': {'emotion': 'neutral', 'score': 0.8974751830101013}, 'Intent': {'intent': 'persuasive', 'score': 0.4130096733570099}}\n",
      "Claimant GPT4 response: {'role': 'GPT4', 'Final Verdict': 'Not Credible', 'Explanation': \"While it is true that the climate has changed throughout Earth's history due to natural factors, the current rate and magnitude of climate change are primarily driven by human activities, particularly the emission of greenhouse gases from burning fossil fuels. This statement lacks the context of the scientific consensus on human-induced climate change, which is crucial for understanding the current situation.\\n(Sentiment: neutral, Intent: persuasive)\", 'Sentiment': {'emotion': 'neutral', 'score': 0.8974751830101013}, 'Intent': {'intent': 'persuasive', 'score': 0.4130096733570099}}\n",
      "{\n",
      "  \"statement\": \"Of course the climate is changing. It always has. It always will\",\n",
      "  \"final_verdict\": \"Not Credible\",\n",
      "  \"explanation\": \"The statement 'Of course the climate is changing. It always has. It always will' is misleading because it implies that current climate change is a natural occurrence similar to past changes. While the climate has indeed changed throughout Earth's history due to natural factors, the current changes are primarily driven by human activities, such as the burning of fossil fuels, which is supported by overwhelming scientific evidence. This statement lacks the necessary context to accurately convey the unique and unprecedented nature of current climate change.\",\n",
      "  \"confidence\": \"high\",\n",
      "  \"risk_level\": \"medium\",\n",
      "  \"debate_rounds\": 1,\n",
      "  \"claimant_responses\": [\n",
      "    {\n",
      "      \"role\": \"Support\",\n",
      "      \"Final Verdict\": \"Not Credible\",\n",
      "      \"Explanation\": \"The statement 'Of course the climate is changing. It always has. It always will' is misleading because it implies that current climate change is a natural occurrence similar to past changes. While the climate has indeed changed throughout Earth's history due to natural factors, the current changes are primarily driven by human activities, such as the burning of fossil fuels, which is supported by overwhelming scientific evidence. This statement lacks the necessary context to accurately convey the unique and unprecedented nature of current climate change.\\n(Sentiment: neutral, Intent: persuasive)\",\n",
      "      \"Sentiment\": {\n",
      "        \"emotion\": \"neutral\",\n",
      "        \"score\": 0.8974751830101013\n",
      "      },\n",
      "      \"Intent\": {\n",
      "        \"intent\": \"persuasive\",\n",
      "        \"score\": 0.4130096733570099\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"Denier\",\n",
      "      \"Final Verdict\": \"Not Enough Information\",\n",
      "      \"Explanation\": \"The documents provided do not contain enough information to assess this statement. The statement about climate change being a constant historical process is not directly addressed or supported by the specific content of the documents. The documents focus on current climate change discussions and media reporting rather than historical climate patterns.\\n(Sentiment: neutral, Intent: persuasive)\",\n",
      "      \"Sentiment\": {\n",
      "        \"emotion\": \"neutral\",\n",
      "        \"score\": 0.8974751830101013\n",
      "      },\n",
      "      \"Intent\": {\n",
      "        \"intent\": \"persuasive\",\n",
      "        \"score\": 0.4130096733570099\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"GPT4\",\n",
      "      \"Final Verdict\": \"Not Credible\",\n",
      "      \"Explanation\": \"While it is true that the climate has changed throughout Earth's history due to natural factors, the current rate and magnitude of climate change are primarily driven by human activities, particularly the emission of greenhouse gases from burning fossil fuels. This statement lacks the context of the scientific consensus on human-induced climate change, which is crucial for understanding the current situation.\\n(Sentiment: neutral, Intent: persuasive)\",\n",
      "      \"Sentiment\": {\n",
      "        \"emotion\": \"neutral\",\n",
      "        \"score\": 0.8974751830101013\n",
      "      },\n",
      "      \"Intent\": {\n",
      "        \"intent\": \"persuasive\",\n",
      "        \"score\": 0.4130096733570099\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"verifier_result\": {\n",
      "    \"Final Verdict\": \"Not Credible\",\n",
      "    \"Explanation\": \"The statement 'Of course the climate is changing. It always has. It always will' is misleading because it suggests that current climate changes are natural and similar to past changes. However, the current climate change is primarily driven by human activities, such as the burning of fossil fuels, which is supported by overwhelming scientific evidence. This statement lacks the necessary context to accurately convey the unique and unprecedented nature of current climate change.\",\n",
      "    \"Confidence\": \"high\",\n",
      "    \"RiskLevel\": \"medium\",\n",
      "    \"Debate\": {\n",
      "      \"Status\": \"Closed\",\n",
      "      \"Round\": 1,\n",
      "      \"Follow-Up Questions\": {}\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "statement = \"Of course the climate is changing. It always has. It always will\"\n",
    "truthseekers_result = truthseekers(statement)\n",
    "print(json.dumps(truthseekers_result, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
